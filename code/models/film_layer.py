"""FiLM (Feature-wise Linear Modulation) layer implementation."""

import torch
import torch.nn as nn


class FiLMLayer(nn.Module):
    """
    FiLM Layer for task-conditioned feature modulation.
    
    Given feature map F and condition embedding c, compute:
    FiLM(F) = γ(c) ⊙ F + β(c)
    
    Where γ (gamma/scale) and β (beta/shift) are channel-wise parameters generated from the condition embedding.
    """
    
    def __init__(self, num_features, condition_dim=None, use_affine=True):
        """
        Args:
            num_features: Number of channels in the input feature map
            condition_dim: Dimension of the condition embedding (if None, use task embedding directly)
            use_affine: Whether to use both scale and shift (False uses scale only)
        """
        super().__init__()
        self.num_features = num_features
        self.use_affine = use_affine
        
        # If condition_dim is specified, use MLP to generate gamma and beta
        if condition_dim is not None:
            self.gamma_fc = nn.Sequential(
                nn.Linear(condition_dim, num_features),
                nn.ReLU(),
                nn.Linear(num_features, num_features)
            )
            if use_affine:
                self.beta_fc = nn.Sequential(
                    nn.Linear(condition_dim, num_features),
                    nn.ReLU(),
                    nn.Linear(num_features, num_features)
                )
        else:
            # Otherwise, gamma and beta are learnable parameters directly (one set per task)
            self.gamma_fc = None
            self.beta_fc = None
    
    def forward(self, x, condition=None):
        """
        Args:
            x: Feature map [B, C, H, W]
            condition: Condition vector [B, condition_dim] or [condition_dim]
        
        Returns:
            Modulated feature map [B, C, H, W]
        """
        if self.gamma_fc is not None:
            # Use MLP to generate gamma and beta
            if condition is None:
                raise ValueError("condition must be provided when using MLP-based FiLM")
            
            # Generate gamma
            gamma = self.gamma_fc(condition)  # [B, C] or [C]
            
            # Generate beta
            if self.use_affine:
                beta = self.beta_fc(condition)  # [B, C] or [C]
            else:
                beta = 0
            
            # Adjust dimensions to match feature map
            if gamma.dim() == 1:
                gamma = gamma.view(1, -1, 1, 1)
                if self.use_affine:
                    beta = beta.view(1, -1, 1, 1)
            else:
                gamma = gamma.view(gamma.size(0), -1, 1, 1)
                if self.use_affine:
                    beta = beta.view(beta.size(0), -1, 1, 1)
        else:
            # Use the passed gamma and beta (generated by TaskFiLMGenerator)
            gamma, beta = condition
            
            # Reshape to broadcast over [B, C, H, W]
            if gamma.dim() == 1:
                gamma = gamma.view(1, -1, 1, 1)
            elif gamma.dim() == 2:
                gamma = gamma.view(gamma.size(0), -1, 1, 1)
            
            if beta is not None:
                if beta.dim() == 1:
                    beta = beta.view(1, -1, 1, 1)
                elif beta.dim() == 2:
                    beta = beta.view(beta.size(0), -1, 1, 1)
        
        # Apply FiLM transformation: γ ⊙ x + β
        out = gamma * x
        if self.use_affine:
            out = out + beta
        
        return out


class TaskFiLMGenerator(nn.Module):
    """
    Generate FiLM parameters (gamma and beta) for each task.
    Suitable for scenarios with a fixed number of tasks, where each task has independent learnable parameters.
    """
    
    def __init__(self, task_ids, num_features, use_affine=True):
        """
        Args:
            task_ids: List of task IDs, e.g., ['task1', 'task2', 'task3']
            num_features: Number of feature channels
            use_affine: Whether to use affine transformation (gamma + beta)
        """
        super().__init__()
        self.task_ids = task_ids
        self.num_features = num_features
        self.use_affine = use_affine
        
        # Create learnable gamma parameters for each task
        self.task_gammas = nn.ParameterDict({
            task_id: nn.Parameter(torch.ones(num_features))
            for task_id in task_ids
        })
        
        # Create learnable beta parameters for each task
        if use_affine:
            self.task_betas = nn.ParameterDict({
                task_id: nn.Parameter(torch.zeros(num_features))
                for task_id in task_ids
            })
    
    def forward(self, task_id):
        """
        Args:
            task_id: Task identifier (string)
        
        Returns:
            gamma: [C] Scale parameter
            beta: [C] Shift parameter (if use_affine=True)
        """
        if task_id not in self.task_ids:
            raise ValueError(f"Unknown task_id: {task_id}. Available: {self.task_ids}")
        
        gamma = self.task_gammas[task_id]
        beta = self.task_betas[task_id] if self.use_affine else None
        
        return gamma, beta


class TaskEmbeddingFiLMGenerator(nn.Module):
    """
    Use task embedding to generate FiLM parameters via MLP.
    More flexible and can learn similarities between tasks.
    """
    
    def __init__(self, task_ids, num_features, embedding_dim=64, use_affine=True):
        """
        Args:
            task_ids: List of task IDs
            num_features: Number of feature channels
            embedding_dim: Task embedding dimension
            use_affine: Whether to use affine transformation
        """
        super().__init__()
        self.task_ids = task_ids
        self.num_features = num_features
        self.use_affine = use_affine
        
        # Task embedding table
        self.task_embeddings = nn.Embedding(len(task_ids), embedding_dim)
        
        # Mapping from task ID to index
        self.task_id_to_idx = {task_id: idx for idx, task_id in enumerate(task_ids)}
        
        # MLP to generate gamma
        self.gamma_generator = nn.Sequential(
            nn.Linear(embedding_dim, num_features * 2),
            nn.ReLU(),
            nn.Linear(num_features * 2, num_features)
        )
        
        # MLP to generate beta
        if use_affine:
            self.beta_generator = nn.Sequential(
                nn.Linear(embedding_dim, num_features * 2),
                nn.ReLU(),
                nn.Linear(num_features * 2, num_features)
            )
    
    def forward(self, task_id):
        """
        Args:
            task_id: Task identifier (string)
        
        Returns:
            gamma: [C] Scale parameter
            beta: [C] Shift parameter
        """
        if task_id not in self.task_id_to_idx:
            raise ValueError(f"Unknown task_id: {task_id}")
        
        # Get task embedding
        task_idx = torch.tensor(self.task_id_to_idx[task_id], dtype=torch.long)
        if next(self.parameters()).is_cuda:
            task_idx = task_idx.cuda()
        
        task_emb = self.task_embeddings(task_idx)  # [embedding_dim]
        
        # Generate gamma and beta
        gamma = self.gamma_generator(task_emb)  # [C]
        beta = self.beta_generator(task_emb) if self.use_affine else None
        
        return gamma, beta


class MultiFiLMLayer(nn.Module):
    """
    Multi-level FiLM modulation for modulating outputs from multiple encoder stages.
    Suitable for deep task conditioning.
    """
    
    def __init__(self, task_ids, feature_channels_list, use_affine=True, use_embedding=False, embedding_dim=64):
        """
        Args:
            task_ids: List of task IDs
            feature_channels_list: List of feature channel numbers for each layer, e.g., [96, 192, 384, 768]
            use_affine: Whether to use affine transformation
            use_embedding: Whether to use task embedding generator
            embedding_dim: Embedding dimension (when use_embedding=True)
        """
        super().__init__()
        self.num_stages = len(feature_channels_list)
        
        # Create FiLM generator for each stage
        if use_embedding:
            self.film_generators = nn.ModuleList([
                TaskEmbeddingFiLMGenerator(
                    task_ids=task_ids,
                    num_features=channels,
                    embedding_dim=embedding_dim,
                    use_affine=use_affine
                )
                for channels in feature_channels_list
            ])
        else:
            self.film_generators = nn.ModuleList([
                TaskFiLMGenerator(
                    task_ids=task_ids,
                    num_features=channels,
                    use_affine=use_affine
                )
                for channels in feature_channels_list
            ])
        
        self.film_layers = nn.ModuleList([
            FiLMLayer(num_features=channels, use_affine=use_affine)
            for channels in feature_channels_list
        ])
    
    def forward(self, features_list, task_id):
        """
        Args:
            features_list: List of features, e.g., encoder stage outputs [feat0, feat1, feat2, feat3]
            task_id: Task identifier
        
        Returns:
            List of modulated features
        """
        modulated_features = []
        
        for i, (feat, film_gen, film_layer) in enumerate(
            zip(features_list, self.film_generators, self.film_layers)
        ):
            # Generate FiLM parameters
            gamma, beta = film_gen(task_id)
            
            # Apply FiLM modulation
            modulated_feat = film_layer(feat, condition=(gamma, beta))
            modulated_features.append(modulated_feat)
        
        return modulated_features
